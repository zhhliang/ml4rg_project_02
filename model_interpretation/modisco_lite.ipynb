{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install package modisco-lite\n",
    "# !pip install modisco-lite==2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install package of meme for TOMTOM comparison\n",
    "# !conda install --quiet --yes -c bioconda meme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install package of requests\n",
    "# !conda install -y conda-forge::requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install package of beautifulsoup4\n",
    "# !conda install -y anaconda::beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install openpyxl\n",
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import joblib\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "import argparse\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display, HTML\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path of all the contribution scores, hypothetical contribution scores and onehot encoding of the sequences\n",
    "file_paths = ['Zhihao_Code/contribution_scores/mesoderm/diff_1.3/BestModel_contr_scores_mesoderm.h5',\n",
    "              'Zhihao_Code/contribution_scores/cardiac/diff_1.3/BestModel_contr_scores_cardiac.h5',\n",
    "              'Zhihao_Code/contribution_scores/visceral/diff_1.3/BestModel_contr_scores_visceral.h5',\n",
    "              'Zhihao_Code/contribution_scores/somatic/diff_1.3/BestModel_contr_scores_somatic.h5',\n",
    "              'Zhihao_Code/contribution_scores/mesoderm/diff/BestModel_contr_scores_mesoderm.h5',\n",
    "              'Zhihao_Code/contribution_scores/cardiac/diff/BestModel_contr_scores_cardiac.h5',\n",
    "              'Zhihao_Code/contribution_scores/visceral/diff/BestModel_contr_scores_visceral.h5',\n",
    "              'Zhihao_Code/contribution_scores/somatic/diff/BestModel_contr_scores_somatic.h5',\n",
    "              'Zhihao_Code/contribution_scores/mesoderm/global/BestModel_contr_scores_mesoderm.h5',\n",
    "              'Zhihao_Code/contribution_scores/cardiac/global/BestModel_contr_scores_cardiac.h5',\n",
    "              'Zhihao_Code/contribution_scores/visceral/global/BestModel_contr_scores_visceral.h5',\n",
    "              'Zhihao_Code/contribution_scores/somatic/global/BestModel_contr_scores_somatic.h5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data for mesoderm\n",
    "f = h5py.File(file_paths[0], 'r')\n",
    "actual_contrib_scores = f['actual_contrib_scores/Contrib_scores'][:]\n",
    "hyp_contrib_scores = f['hyp_contrib_scores/Contrib_scores'][:]\n",
    "onehot_samples = f['onehot_samples/cand_x'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        -5.11014878e-05],\n",
       "       [-0.00000000e+00, -6.59241749e-04, -0.00000000e+00,\n",
       "        -0.00000000e+00],\n",
       "       [ 0.00000000e+00, -2.70656019e-04, -0.00000000e+00,\n",
       "        -0.00000000e+00],\n",
       "       ...,\n",
       "       [ 0.00000000e+00,  8.04727781e-04,  0.00000000e+00,\n",
       "        -0.00000000e+00],\n",
       "       [-0.00000000e+00, -4.86345991e-04,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [ 7.60875642e-04,  0.00000000e+00,  0.00000000e+00,\n",
       "        -0.00000000e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_contrib_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.23460350e-04, -1.62347438e-04,  1.00675046e-04,\n",
       "        -5.11014878e-05],\n",
       "       [-2.94170404e-05, -6.59241749e-04, -5.81847795e-04,\n",
       "        -7.00870951e-05],\n",
       "       [ 4.88050311e-04, -2.70656019e-04, -1.01336700e-04,\n",
       "        -9.37179371e-04],\n",
       "       ...,\n",
       "       [ 3.60038335e-04,  8.04727781e-04,  3.29026225e-04,\n",
       "        -1.11727022e-05],\n",
       "       [-1.94355671e-04, -4.86345991e-04,  8.26081814e-05,\n",
       "         1.04566134e-04],\n",
       "       [ 7.60875642e-04,  3.56152246e-04,  1.96030851e-06,\n",
       "        -1.88279053e-04]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp_contrib_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each file and create the npz files for modisco\n",
    "for file_path in file_paths:\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Load actual contribution scores\n",
    "        actual_contrib_scores = f['actual_contrib_scores/Contrib_scores'][:]\n",
    "        \n",
    "        # Load hypothetical contribution scores\n",
    "        hyp_contrib_scores = f['hyp_contrib_scores/Contrib_scores'][:]\n",
    "        \n",
    "        # Load one-hot encoded samples\n",
    "        onehot_samples = f['onehot_samples/cand_x'][:]\n",
    "\n",
    "        # Check and reshape the data if necessary\n",
    "        if onehot_samples.shape[1] != 4:\n",
    "            onehot_samples = np.transpose(onehot_samples, (0, 2, 1))\n",
    "\n",
    "        if actual_contrib_scores.shape[1] != 4:\n",
    "            actual_contrib_scores = np.transpose(actual_contrib_scores, (0, 2, 1))\n",
    "\n",
    "        if hyp_contrib_scores.shape[1] != 4:\n",
    "            hyp_contrib_scores = np.transpose(hyp_contrib_scores, (0, 2, 1))    \n",
    "\n",
    "        np.savez(os.path.dirname(file_path) + '/attributions_scores.npz', hyp_contrib_scores)\n",
    "        \n",
    "        np.savez(os.path.dirname(file_path) + '/onehot_samples.npz', onehot_samples)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: modisco motifs [-h] [-s SEQUENCES] [-a ATTRIBUTIONS] [-i H5PY] -n\n",
      "                      MAX_SEQLETS [-l N_LEIDEN] [-w WINDOW] [-o OUTPUT] [-v]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -s SEQUENCES, --sequences SEQUENCES\n",
      "                        A .npy or .npz file containing the one-hot encoded\n",
      "                        sequences.\n",
      "  -a ATTRIBUTIONS, --attributions ATTRIBUTIONS\n",
      "                        A .npy or .npz file containing the hypothetical\n",
      "                        attributions, i.e., the attributions for all\n",
      "                        nucleotides at all positions.\n",
      "  -i H5PY, --h5py H5PY  A legacy h5py file containing the one-hot encoded\n",
      "                        sequences and shap scores.\n",
      "  -n MAX_SEQLETS, --max_seqlets MAX_SEQLETS\n",
      "                        The maximum number of seqlets per metacluster.\n",
      "  -l N_LEIDEN, --n_leiden N_LEIDEN\n",
      "                        The number of Leiden clusterings to perform with\n",
      "                        different random seeds.\n",
      "  -w WINDOW, --window WINDOW\n",
      "                        The window surrounding the peak center that will be\n",
      "                        considered for motif discovery.\n",
      "  -o OUTPUT, --output OUTPUT\n",
      "                        The path to the output file.\n",
      "  -v, --verbose         Controls the amount of output from the code.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "modisco motifs --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: modisco report [-h] -i H5PY -o OUTPUT [-s SUFFIX] [-m MEME_DB]\n",
      "                      [-n N_MATCHES]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -i H5PY, --h5py H5PY  An HDF5 file containing the output from modiscolite.\n",
      "  -o OUTPUT, --output OUTPUT\n",
      "                        A directory to put the output results including the\n",
      "                        html report.\n",
      "  -s SUFFIX, --suffix SUFFIX\n",
      "                        The suffix to add to the beginning of images. Should\n",
      "                        be equal to the output if using a Jupyter notebook.\n",
      "  -m MEME_DB, --meme_db MEME_DB\n",
      "                        A MEME file containing motifs.\n",
      "  -n N_MATCHES, --n_matches N_MATCHES\n",
      "                        The number of top TOMTOM matches to include in the\n",
      "                        report.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "modisco report --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rund medisco-lite\n",
    "for file_path in file_paths:\n",
    "\n",
    "    # Get current path\n",
    "    dir_main_path = os.getcwd() \n",
    "    # Extract the directory from the file path\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "\n",
    "    # Change to the directory\n",
    "    os.chdir(dir_path)\n",
    "\n",
    "    # Run the modisco motifs command\n",
    "    subprocess.run([\n",
    "        'modisco', 'motifs',\n",
    "        '-s', 'onehot_samples.npz',\n",
    "        '-a', 'attributions_scores.npz',\n",
    "        '-n', '50000',\n",
    "        '-o', 'modisco_test_results.h5'\n",
    "    ])\n",
    "    \n",
    "    # Run the modisco report command\n",
    "    subprocess.run([\n",
    "        'modisco', 'report',\n",
    "        '-i', 'modisco_test_results.h5',\n",
    "        '-o', 'report/',\n",
    "        '-s', 'report/'\n",
    "    ])\n",
    "\n",
    "    # copy 'html' file one level above\n",
    "    subprocess.run([\n",
    "        'cp', 'report/motifs.html', '.'\n",
    "    ])\n",
    "\n",
    "    # Run the modisco repor compare command\n",
    "    subprocess.run([\n",
    "        'modisco', 'report',\n",
    "        '-i', 'modisco_test_results.h5',\n",
    "        '-o', 'report_meme/',\n",
    "        '-s', 'report_meme/',\n",
    "        '-m', '../../../../Ying_code/modisco/JASPAR2024_CORE_insects_non-redundant_pfms_meme.txt'\n",
    "    ])\n",
    "\n",
    "    # copy 'html' file one level above\n",
    "    subprocess.run([\n",
    "        'cp', 'report_meme/motifs.html', 'motifs_meme.html'\n",
    "    ])\n",
    "\n",
    "    # come back to the main path\n",
    "    os.chdir(dir_main_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to annotate the modisco discovered motifs, adding TFs names and class information, then capture the html report information to a csv file\n",
    "def add_tf_names_to_html_and_extract_table(html_report, list_output_path, table_output_path):\n",
    "    def get_transcription_factor_info(jaspar_id):\n",
    "        url = f\"http://jaspar.genereg.net/api/v1/matrix/{jaspar_id}/\"\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            tf_name = data.get('name')\n",
    "            tf_class = data.get('class')\n",
    "            return tf_name, tf_class\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    pattern = r'(?<=<td>)MA\\d+\\.\\d+(?=</td>)'\n",
    "\n",
    "    in_html = html_report\n",
    "    out_html = os.path.join(os.path.dirname(in_html), os.path.basename(in_html).replace('.html', '_tfnames.html'))\n",
    "    tf_name_list = []\n",
    "    tf_class_list = []\n",
    "\n",
    "    with open(in_html, 'r') as infile:\n",
    "        with open(out_html, 'w') as outfile:\n",
    "            lines = infile.readlines()\n",
    "            for line in lines:\n",
    "                matches = re.findall(pattern, line)\n",
    "                if len(matches) == 1:\n",
    "                    jaspar_id = matches[0]\n",
    "                    tf_name, tf_class = get_transcription_factor_info(jaspar_id)\n",
    "                    tf_name_list.append(f\"{jaspar_id} - {tf_name}\")\n",
    "                    tf_class_list.append(tf_class)\n",
    "                    new_entry = f\"{jaspar_id} - {tf_name}\"\n",
    "                    newline = line.replace(jaspar_id, new_entry)\n",
    "                else:\n",
    "                    newline = line\n",
    "                outfile.write(newline)\n",
    "\n",
    "    print(f\"Processed {html_report} and created {out_html}\")\n",
    "    print(tf_name_list, \"(\", len(tf_name_list), \" entries)\")\n",
    "\n",
    "    # Replace None with empty string\n",
    "    tf_name_list = [\"\" if i is None else i for i in tf_name_list]\n",
    "    tf_class_list = [\"\" if i is None else i for i in tf_class_list]\n",
    "\n",
    "    # Write the lists to a file\n",
    "    with open(list_output_path, 'w') as f:\n",
    "        for name, cls in zip(tf_name_list, tf_class_list):\n",
    "            f.write(f\"{name}\\t{cls}\\n\")\n",
    "\n",
    "    # Extract table data\n",
    "    with open(out_html, 'r') as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "    table = soup.find('table', {'class': 'dataframe'})\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    table_data = []\n",
    "    headers = [header.text for header in rows[0].find_all('th')]\n",
    "    \n",
    "    # Exclude unwanted columns\n",
    "    columns_to_include = [\"pattern\", \"num_seqlets\", \"match0\", \"qval0\", \"match1\", \"qval1\", \"match2\", \"qval2\"]\n",
    "    columns_index = [headers.index(col) for col in columns_to_include]\n",
    "\n",
    "    for row in rows[1:]:\n",
    "        cols = row.find_all('td')\n",
    "        row_data = [cols[i].text for i in columns_index]\n",
    "        table_data.append(row_data)\n",
    "\n",
    "    df = pd.DataFrame(table_data, columns=columns_to_include)\n",
    "    df.to_csv(table_output_path, index=False)\n",
    "\n",
    "    # Display the modified HTML content\n",
    "    # display(HTML(str(soup)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Zhihao_Code/contribution_scores/mesoderm/diff_1.3/motifs_meme.html and created Zhihao_Code/contribution_scores/mesoderm/diff_1.3/motifs_meme_tfnames.html\n",
      "['MA0249.3 - twi', 'MA2227.1 - esg', 'MA0086.3 - sna', 'MA1459.2 - M1BP', 'MA2322.1 - ZIPIC', 'MA2205.1 - CG7928', 'MA2191.1 - amos', 'MA0249.3 - twi', 'MA2268.1 - sage', 'MA2282.1 - SREBP', 'MA2244.1 - Hr39', 'MA1461.2 - sv', 'MA1700.1 - Clamp', 'MA0205.3 - Trl', 'MA2198.1 - Blimp-1', 'MA1700.1 - Clamp', 'MA0205.3 - Trl', 'MA2107.1 - cg'] ( 18  entries)\n",
      "Processed Zhihao_Code/contribution_scores/cardiac/diff_1.3/motifs_meme.html and created Zhihao_Code/contribution_scores/cardiac/diff_1.3/motifs_meme_tfnames.html\n",
      "['MA0536.2 - pnr', 'MA1456.2 - Dref', 'MA0529.3 - BEAF-32', 'MA0247.3 - tin', 'MA0253.1 - vnd', 'MA0211.1 - bap', 'MA2299.1 - CG4854', 'MA2322.1 - ZIPIC', 'MA1459.2 - M1BP', 'MA2191.1 - amos', 'MA2303.1 - crp', 'MA2268.1 - sage', 'MA2322.1 - ZIPIC', 'MA2271.1 - sens', 'MA1459.2 - M1BP', 'MA2314.1 - hr3', 'MA1700.1 - Clamp', 'MA0452.3 - Kr', 'MA2264.1 - Rel', 'MA1459.2 - M1BP', 'MA1836.2 - dsx', 'MA2309.1 - ewg', 'MA2189.1 - Adf1', 'MA0535.1 - Mad', 'MA1700.1 - Clamp', 'MA0205.3 - Trl', 'MA2309.1 - ewg', 'MA2107.1 - cg', 'MA2217.1 - dpn', 'MA2240.1 - Hey', 'MA0249.3 - twi', 'MA0253.1 - vnd', 'MA0247.3 - tin'] ( 33  entries)\n",
      "Processed Zhihao_Code/contribution_scores/visceral/diff_1.3/motifs_meme.html and created Zhihao_Code/contribution_scores/visceral/diff_1.3/motifs_meme_tfnames.html\n",
      "[] ( 0  entries)\n",
      "Processed Zhihao_Code/contribution_scores/somatic/diff_1.3/motifs_meme.html and created Zhihao_Code/contribution_scores/somatic/diff_1.3/motifs_meme_tfnames.html\n",
      "['MA1700.1 - Clamp', 'MA0205.3 - Trl', 'MA2198.1 - Blimp-1', 'MA2263.1 - Poxm', 'MA2322.1 - ZIPIC', 'MA2322.1 - ZIPIC', 'MA2266.1 - Rfx', 'MA2220.1 - E(spl)mbeta-HLH', 'MA2217.1 - dpn', 'MA1700.1 - Clamp', 'MA1700.1 - Clamp', 'MA2314.1 - hr3', 'MA1459.2 - M1BP'] ( 13  entries)\n",
      "Processed Zhihao_Code/contribution_scores/mesoderm/diff/motifs_meme.html and created Zhihao_Code/contribution_scores/mesoderm/diff/motifs_meme_tfnames.html\n",
      "['MA0249.3 - twi', 'MA2227.1 - esg', 'MA0086.3 - sna', 'MA1700.1 - Clamp', 'MA0205.3 - Trl', 'MA2198.1 - Blimp-1', 'MA2289.1 - toy', 'MA1461.2 - sv', 'MA0533.1 - su(Hw)'] ( 9  entries)\n",
      "Processed Zhihao_Code/contribution_scores/cardiac/diff/motifs_meme.html and created Zhihao_Code/contribution_scores/cardiac/diff/motifs_meme_tfnames.html\n",
      "['MA2309.1 - ewg', 'MA2217.1 - dpn', 'MA2304.1 - cwo', 'MA0247.3 - tin', 'MA0253.1 - vnd', 'MA2257.1 - Mitf', 'MA2289.1 - toy', 'MA1836.2 - dsx', 'MA2273.1 - Sox102F'] ( 9  entries)\n",
      "Processed Zhihao_Code/contribution_scores/visceral/diff/motifs_meme.html and created Zhihao_Code/contribution_scores/visceral/diff/motifs_meme_tfnames.html\n",
      "[] ( 0  entries)\n",
      "Processed Zhihao_Code/contribution_scores/somatic/diff/motifs_meme.html and created Zhihao_Code/contribution_scores/somatic/diff/motifs_meme_tfnames.html\n",
      "['MA1700.1 - Clamp', 'MA0205.3 - Trl', 'MA2309.1 - ewg', 'MA2322.1 - ZIPIC', 'MA2294.1 - Xrp1', 'MA2225.1 - erm', 'MA0535.1 - Mad', 'MA2309.1 - ewg', 'MA2280.1 - Spps'] ( 9  entries)\n",
      "Processed Zhihao_Code/contribution_scores/mesoderm/global/motifs_meme.html and created Zhihao_Code/contribution_scores/mesoderm/global/motifs_meme_tfnames.html\n",
      "['MA0529.3 - BEAF-32', 'MA1456.2 - Dref', 'MA0536.2 - pnr', 'MA1459.2 - M1BP', 'MA2314.1 - hr3', 'MA2259.1 - org-1', 'MA2322.1 - ZIPIC', 'MA2205.1 - CG7928', 'MA1458.2 - Hsf', 'MA1700.1 - Clamp', 'MA0205.3 - Trl', 'MA2198.1 - Blimp-1', 'MA2299.1 - CG4854', 'MA1459.2 - M1BP', 'MA2322.1 - ZIPIC', 'MA2308.1 - da', 'MA2317.1 - nau', 'MA2321.1 - tx', 'MA2205.1 - CG7928', 'MA2322.1 - ZIPIC', 'MA0452.3 - Kr', 'MA2107.1 - cg', 'MA0249.3 - twi', 'MA2240.1 - Hey', 'MA0536.2 - pnr', 'MA1456.2 - Dref', 'MA0529.3 - BEAF-32'] ( 27  entries)\n",
      "Processed Zhihao_Code/contribution_scores/cardiac/global/motifs_meme.html and created Zhihao_Code/contribution_scores/cardiac/global/motifs_meme_tfnames.html\n",
      "['MA1456.2 - Dref', 'MA0529.3 - BEAF-32', 'MA0536.2 - pnr', 'MA2322.1 - ZIPIC', 'MA2205.1 - CG7928', 'MA2288.1 - tj', 'MA1700.1 - Clamp', 'MA0205.3 - Trl', 'MA2198.1 - Blimp-1', 'MA1459.2 - M1BP', 'MA2314.1 - hr3', 'MA2259.1 - org-1', 'MA2299.1 - CG4854', 'MA1459.2 - M1BP', 'MA1461.2 - sv', 'MA2308.1 - da', 'MA2317.1 - nau', 'MA2321.1 - tx', 'MA0249.3 - twi', 'MA2203.1 - CG12605', 'MA2240.1 - Hey', 'MA2202.1 - byn', 'MA0533.1 - su(Hw)', 'MA2216.1 - Doc2'] ( 24  entries)\n",
      "Processed Zhihao_Code/contribution_scores/visceral/global/motifs_meme.html and created Zhihao_Code/contribution_scores/visceral/global/motifs_meme_tfnames.html\n",
      "['MA0529.3 - BEAF-32', 'MA1456.2 - Dref', 'MA0536.2 - pnr', 'MA1459.2 - M1BP', 'MA2314.1 - hr3', 'MA2284.1 - ss', 'MA2322.1 - ZIPIC', 'MA2205.1 - CG7928', 'MA2276.1 - Sox21a', 'MA1700.1 - Clamp', 'MA0205.3 - Trl', 'MA2198.1 - Blimp-1', 'MA2299.1 - CG4854', 'MA1459.2 - M1BP', 'MA2234.1 - Fer2', 'MA2308.1 - da', 'MA2317.1 - nau', 'MA2321.1 - tx', 'MA0536.2 - pnr', 'MA2205.1 - CG7928', 'MA0529.3 - BEAF-32', 'MA2205.1 - CG7928', 'MA0186.1 - Dfd', 'MA0203.1 - Scr'] ( 24  entries)\n",
      "Processed Zhihao_Code/contribution_scores/somatic/global/motifs_meme.html and created Zhihao_Code/contribution_scores/somatic/global/motifs_meme_tfnames.html\n",
      "['MA1459.2 - M1BP', 'MA2314.1 - hr3', 'MA2259.1 - org-1', 'MA0536.2 - pnr', 'MA0529.3 - BEAF-32', 'MA1456.2 - Dref', 'MA2322.1 - ZIPIC', 'MA2289.1 - toy', 'MA2205.1 - CG7928', 'MA1700.1 - Clamp', 'MA0205.3 - Trl', 'MA2249.1 - ken', 'MA1700.1 - Clamp', 'MA0205.3 - Trl', 'MA1459.2 - M1BP', 'MA2299.1 - CG4854', 'MA1459.2 - M1BP', 'MA2322.1 - ZIPIC', 'MA2308.1 - da', 'MA2317.1 - nau', 'MA2321.1 - tx', 'MA2322.1 - ZIPIC', 'MA2225.1 - erm', 'MA0237.2 - pan', 'MA2205.1 - CG7928', 'MA0535.1 - Mad', 'MA0022.1 - dl', 'MA0536.2 - pnr', 'MA1459.2 - M1BP', 'MA0529.3 - BEAF-32', 'MA0529.3 - BEAF-32', 'MA1456.2 - Dref', 'MA0536.2 - pnr', 'MA0531.2 - CTCF', 'MA0213.1 - brk', 'MA2261.1 - phol', 'MA1456.2 - Dref', 'MA0536.2 - pnr', 'MA0529.3 - BEAF-32'] ( 39  entries)\n"
     ]
    }
   ],
   "source": [
    "# loop the dataset and run the function above\n",
    "all_data = {}\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # Extract the directory from the file path\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    folder_name = os.path.basename(dir_path)\n",
    "    \n",
    "    # Define the path to the HTML report\n",
    "    html_report_path = os.path.join(dir_path, 'motifs_meme.html')\n",
    "    list_output_path = os.path.join(dir_path, 'motifs_meme_tfnames.txt')\n",
    "    table_output_path = os.path.join(dir_path, 'modisco_report_table.csv')\n",
    "    \n",
    "    # Check if the HTML report exists\n",
    "    if os.path.exists(html_report_path):\n",
    "        # Run the add_tf_names_to_html function on the HTML report\n",
    "        add_tf_names_to_html_and_extract_table(html_report_path, list_output_path, table_output_path)\n",
    "\n",
    "        # Read the generated CSV file into a DataFrame\n",
    "        df = pd.read_csv(table_output_path)\n",
    "        all_data[folder_name] = df\n",
    "    else:\n",
    "        print(f\"HTML report not found at {html_report_path}\")\n",
    "\n",
    "# Write all data to a single Excel file with each sheet named after the folder\n",
    "output_excel_path = 'merged_modisco_report_tables.xlsx'\n",
    "with pd.ExcelWriter(output_excel_path) as writer:\n",
    "    for sheet_name, df in all_data.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original merged Excel file saved to merged_qvalues_report.xlsx\n",
      "Merged Excel file with single qval columns saved to merged_single_qvalue_report.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Extract the directory names for diff and global datasets\n",
    "diff_1_3_datasets = ['mesoderm', 'cardiac', 'visceral', 'somatic']\n",
    "diff_datasets = ['mesoderm', 'cardiac', 'visceral', 'somatic']\n",
    "global_datasets = ['mesoderm', 'cardiac', 'visceral', 'somatic']\n",
    "\n",
    "# Function to read CSV files and convert data into a structured format\n",
    "def read_csv_and_extract_data(file_paths, datasets):\n",
    "    data = {}\n",
    "    class_info = {}\n",
    "    for path in file_paths:\n",
    "        dir_path = os.path.dirname(path)\n",
    "        dataset = os.path.basename(os.path.dirname(os.path.dirname(path)))\n",
    "        csv_path = os.path.join(dir_path, 'modisco_report_table.csv')\n",
    "        class_path = os.path.join(dir_path, 'motifs_meme_tfnames.txt')\n",
    "        \n",
    "        # Read the class information from motifs_meme_tfnames.txt\n",
    "        if os.path.exists(class_path):\n",
    "            with open(class_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    name, cls = line.strip().split('\\t')\n",
    "                    class_info[name] = cls\n",
    "        \n",
    "        if os.path.exists(csv_path):\n",
    "            df = pd.read_csv(csv_path)\n",
    "            for idx, row in df.iterrows():\n",
    "                for match_col in ['match0', 'match1', 'match2']:\n",
    "                    motif_name = row[match_col]\n",
    "                    qval_col = f'qval{match_col[-1]}'\n",
    "                    if pd.notna(motif_name):\n",
    "                        if motif_name not in data:\n",
    "                            data[motif_name] = {dataset: {qval_col: row[qval_col]}}\n",
    "                        else:\n",
    "                            if dataset not in data[motif_name]:\n",
    "                                data[motif_name][dataset] = {qval_col: row[qval_col]}\n",
    "                            else:\n",
    "                                data[motif_name][dataset][qval_col] = row[qval_col]\n",
    "    return data, class_info\n",
    "\n",
    "# Extract data from the file paths\n",
    "\n",
    "diff_1_3_data, diff_1_3_class_info = read_csv_and_extract_data(file_paths[:4], diff_1_3_datasets)\n",
    "diff_data, diff_class_info = read_csv_and_extract_data(file_paths[4:8], diff_datasets)\n",
    "global_data, global_class_info = read_csv_and_extract_data(file_paths[8:], global_datasets)\n",
    "\n",
    "# Merge class info dictionaries\n",
    "all_class_info = {**diff_1_3_class_info, **diff_class_info, **global_class_info}\n",
    "\n",
    "# Function to convert the extracted data into a DataFrame\n",
    "def convert_to_dataframe(data, datasets, class_info):\n",
    "    all_motifs = sorted([k for k in data.keys() if isinstance(k, str)])\n",
    "    result_df = pd.DataFrame(index=all_motifs, columns=pd.MultiIndex.from_product([datasets, ['qval0', 'qval1', 'qval2']]))\n",
    "    \n",
    "    classes = []\n",
    "    for motif in all_motifs:\n",
    "        for dataset in datasets:\n",
    "            for qval in ['qval0', 'qval1', 'qval2']:\n",
    "                if dataset in data[motif] and qval in data[motif][dataset]:\n",
    "                    result_df.loc[motif, (dataset, qval)] = data[motif][dataset][qval]\n",
    "                else:\n",
    "                    result_df.loc[motif, (dataset, qval)] = np.nan\n",
    "        # Add class information\n",
    "        classes.append(class_info.get(motif, \"\"))\n",
    "    \n",
    "    result_df['class'] = classes\n",
    "    return result_df\n",
    "\n",
    "# Convert the extracted data to DataFrames with class information\n",
    "diff_1_3_df = convert_to_dataframe(diff_1_3_data, diff_1_3_datasets, all_class_info)\n",
    "diff_df = convert_to_dataframe(diff_data, diff_datasets, all_class_info)\n",
    "global_df = convert_to_dataframe(global_data, global_datasets, all_class_info)\n",
    "\n",
    "# Write the original DataFrames to a new Excel file\n",
    "output_excel_path = 'merged_qvalues_report.xlsx'\n",
    "with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:\n",
    "    diff_1_3_df.to_excel(writer, sheet_name='diff_1_3_qvalues')\n",
    "    diff_df.to_excel(writer, sheet_name='diff_qvalues')\n",
    "    global_df.to_excel(writer, sheet_name='global_qvalues')\n",
    "\n",
    "print(f\"Original merged Excel file saved to {output_excel_path}\")\n",
    "\n",
    "# Function to merge qval columns into one by selecting the lowest value\n",
    "def merge_qval_columns(df):\n",
    "    merged_df = pd.DataFrame(index=df.index)\n",
    "    for cell_type in df.columns.levels[0]:\n",
    "        if cell_type != 'class':\n",
    "            qvals = df[cell_type].min(axis=1)  # Select the lowest value from qval0, qval1, and qval2\n",
    "            merged_df[cell_type] = qvals\n",
    "    merged_df['class'] = df['class']  # Copy the class column\n",
    "    return merged_df\n",
    "\n",
    "# Merge qval columns into one for diff and global DataFrames\n",
    "diff_1_3_df_merged = merge_qval_columns(diff_1_3_df)\n",
    "diff_df_merged = merge_qval_columns(diff_df)\n",
    "global_df_merged = merge_qval_columns(global_df)\n",
    "\n",
    "# Ensure the correct column order\n",
    "diff_1_3_df_merged = diff_1_3_df_merged[diff_1_3_datasets + ['class']]\n",
    "diff_df_merged = diff_df_merged[diff_datasets + ['class']]\n",
    "global_df_merged = global_df_merged[global_datasets + ['class']]\n",
    "\n",
    "# Write the merged DataFrames to a new Excel file\n",
    "output_excel_path_merged = 'merged_single_qvalue_report.xlsx'\n",
    "with pd.ExcelWriter(output_excel_path_merged, engine='openpyxl') as writer:\n",
    "    diff_1_3_df_merged.to_excel(writer, sheet_name='diff_1_3_qvalues')\n",
    "    diff_df_merged.to_excel(writer, sheet_name='diff_qvalues')\n",
    "    global_df_merged.to_excel(writer, sheet_name='global_qvalues')\n",
    "\n",
    "print(f\"Merged Excel file with single qval columns saved to {output_excel_path_merged}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepSTARR2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
